{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the required packages\n",
    "! pip install tensorflow scikit-image tqdm matplotlib CairoSVG svglib reportlab keras --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_curve, roc_curve, auc\n",
    "from skimage import io, transform\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Data Ingestion\n",
    "\n",
    "def load_and_preprocess_image(file_path, target_size=(32, 32)):\n",
    "    \"\"\"Load and preprocess a single satellite image.\"\"\"\n",
    "    img = io.imread(file_path)\n",
    "    img = transform.resize(img, target_size, anti_aliasing=True)\n",
    "    img = img.astype(np.float32) / 255.0\n",
    "    return img\n",
    "\n",
    "def load_dataset(data_dir, target_size=(32, 32)):\n",
    "    \"\"\"Load all images and labels from the data directory.\"\"\"\n",
    "    images = []\n",
    "    labels = []\n",
    "    \n",
    "    print(f\"Searching for data in: {data_dir}\")\n",
    "    \n",
    "    if not os.path.exists(data_dir):\n",
    "        print(f\"Error: Directory {data_dir} does not exist.\")\n",
    "        return np.array(images), np.array(labels)\n",
    "    \n",
    "    # Check if data_dir is a single timestamp directory or contains multiple timestamp directories\n",
    "    if all(f.startswith('channel_') or f == 'label.png' for f in os.listdir(data_dir)):\n",
    "        # Single timestamp directory\n",
    "        directories = [data_dir]\n",
    "    else:\n",
    "        # Multiple timestamp directories\n",
    "        directories = [os.path.join(data_dir, d) for d in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, d))]\n",
    "    \n",
    "    for directory in directories:\n",
    "        timestamp_images = []\n",
    "        for i in range(12):\n",
    "            img_path = os.path.join(directory, f'channel_{i}.png')\n",
    "            if os.path.exists(img_path):\n",
    "                img = load_and_preprocess_image(img_path, target_size)\n",
    "                timestamp_images.append(img)\n",
    "                print(f\"Loaded channel {i}: {img_path}\")\n",
    "            else:\n",
    "                print(f\"Missing image: {img_path}\")\n",
    "        \n",
    "        if len(timestamp_images) == 12:\n",
    "            image_stack = np.stack(timestamp_images, axis=-1)\n",
    "            images.append(image_stack)\n",
    "            print(f\"Successfully created image stack with shape: {image_stack.shape}\")\n",
    "            \n",
    "            label_path = os.path.join(directory, 'label.png')\n",
    "            if os.path.exists(label_path):\n",
    "                label = io.imread(label_path)\n",
    "                label = load_and_preprocess_image(label_path, target_size)\n",
    "                labels.append(label)\n",
    "                print(f\"Loaded label: {label_path}\")\n",
    "            else:\n",
    "                print(f\"Missing label: {label_path}\")\n",
    "        else:\n",
    "            print(f\"Incomplete channel set in {directory}. Found {len(timestamp_images)} channels instead of 12.\")\n",
    "    \n",
    "    print(f\"Total image stacks loaded: {len(images)}\")\n",
    "    print(f\"Total labels loaded: {len(labels)}\")\n",
    "    \n",
    "    return np.array(images), np.array(labels)\n",
    "\n",
    "def prepare_data_for_model(images, labels, test_size=0.2, validation_split=0.2):\n",
    "    \"\"\"Prepare the data for model training, including train/val/test split.\"\"\"\n",
    "    print(f\"Images shape: {images.shape}\")\n",
    "    print(f\"Labels shape before reshape: {labels.shape}\")\n",
    "    print(f\"Labels unique values: {np.unique(labels)}\")\n",
    "\n",
    "    if labels.size == 0:\n",
    "        raise ValueError(\"Labels array is empty. Please check the data loading process.\")\n",
    "\n",
    "    labels = labels.reshape(labels.shape[0], -1)\n",
    "    print(f\"Labels shape after reshape: {labels.shape}\")\n",
    "\n",
    "    n_samples = len(images)\n",
    "    if n_samples < 3:\n",
    "        print(f\"Warning: Only {n_samples} samples found. Splitting may not be possible.\")\n",
    "        return images, images, images, labels, labels, labels\n",
    "\n",
    "    if n_samples < 10:\n",
    "        print(f\"Warning: Only {n_samples} samples found. Using a 60/20/20 split.\")\n",
    "        test_size = 0.2\n",
    "        validation_split = 0.25  # 25% of 80% is 20% of the total\n",
    "\n",
    "    X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "        images, labels, test_size=test_size, random_state=42)\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_train_val, y_train_val, test_size=validation_split, random_state=42)\n",
    "\n",
    "    print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "    print(f\"X_val shape: {X_val.shape}, y_val shape: {y_val.shape}\")\n",
    "    print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n",
    "\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "def data_generator(images, labels, class_weights, batch_size=32):\n",
    "    num_samples = len(images)\n",
    "    while True:\n",
    "        indices = np.random.permutation(num_samples)\n",
    "        for start in range(0, num_samples, batch_size):\n",
    "            end = min(start + batch_size, num_samples)\n",
    "            batch_indices = indices[start:end]\n",
    "            batch_images = images[batch_indices]\n",
    "            batch_labels = labels[batch_indices]\n",
    "            print(\"Batch images shape:\", batch_images.shape)\n",
    "            print(\"Batch labels shape:\", batch_labels.shape)\n",
    "            batch_weights = np.array([class_weights[label] for label in batch_labels.flatten()])\n",
    "            yield batch_images, batch_labels, batch_weights\n",
    "\n",
    "# Model Architecture\n",
    "\n",
    "def create_lightning_cnn(input_shape=(32, 32, 12), num_classes=1024):\n",
    "    model = models.Sequential([\n",
    "        layers.Conv2D(16, (3, 3), activation='relu', padding='same', input_shape=input_shape),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        \n",
    "        layers.Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        \n",
    "        layers.Flatten(),\n",
    "        \n",
    "        layers.Dense(512, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.5),\n",
    "        \n",
    "        layers.Dense(num_classes, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def compile_model(model, learning_rate=1e-3, decay=0.9):\n",
    "    lr_schedule = optimizers.schedules.InverseTimeDecay(\n",
    "        initial_learning_rate=learning_rate,\n",
    "        decay_steps=1000,\n",
    "        decay_rate=decay\n",
    "    )\n",
    "    optimizer = optimizers.Adam(learning_rate=lr_schedule)\n",
    "    \n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Model Training\n",
    "\n",
    "def train_model(model, X_train, y_train, X_val, y_val, batch_size=128, epochs=3000):\n",
    "    # Calculate class weights\n",
    "    unique, counts = np.unique(y_train.flatten(), return_counts=True)\n",
    "    class_weights = dict(zip(unique, len(y_train.flatten()) / (len(unique) * counts)))\n",
    "\n",
    "    early_stopping = EarlyStopping(patience=50, restore_best_weights=True)\n",
    "    reduce_lr = ReduceLROnPlateau(factor=0.2, patience=20)\n",
    "\n",
    "    train_gen = data_generator(X_train, y_train, class_weights, batch_size)\n",
    "    val_gen = data_generator(X_val, y_val, class_weights, batch_size)\n",
    "\n",
    "    history = model.fit(\n",
    "        train_gen,\n",
    "        steps_per_epoch=len(X_train) // batch_size,\n",
    "        epochs=epochs,\n",
    "        validation_data=val_gen,\n",
    "        validation_steps=len(X_val) // batch_size,\n",
    "        callbacks=[early_stopping, reduce_lr]\n",
    "    )\n",
    "    \n",
    "    return history\n",
    "\n",
    "def create_custom_cnn(input_shape=(32, 12, 4, 12), num_classes=1024):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    \n",
    "    # Reshape the input to combine the 12 and 4 dimensions\n",
    "    x = layers.Reshape((32, 12, 48))(inputs)\n",
    "    \n",
    "    x = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling2D((2, 2))(x)\n",
    "    \n",
    "    x = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling2D((2, 2))(x)\n",
    "    \n",
    "    x = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    \n",
    "    x = layers.Dense(512, activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    \n",
    "    outputs = layers.Dense(num_classes, activation='sigmoid')(x)\n",
    "    \n",
    "    model = models.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "# Performance Visualization\n",
    "\n",
    "def plot_training_history(history):\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 12))\n",
    "    \n",
    "    ax1.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    ax1.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    ax1.set_title('Model Accuracy')\n",
    "    ax1.set_ylabel('Accuracy')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.legend()\n",
    "    \n",
    "    ax2.plot(history.history['loss'], label='Training Loss')\n",
    "    ax2.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    ax2.set_title('Model Loss')\n",
    "    ax2.set_ylabel('Loss')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_precision_recall_curve(y_true, y_pred):\n",
    "    precision, recall, _ = precision_recall_curve(y_true.ravel(), y_pred.ravel())\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(recall, precision, label='Precision-Recall Curve')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Precision-Recall Curve')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def plot_roc_curve(y_true, y_pred):\n",
    "    fpr, tpr, _ = roc_curve(y_true.ravel(), y_pred.ravel())\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {roc_auc:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def visualize_predictions(y_true, y_pred, num_samples=5):\n",
    "    fig, axes = plt.subplots(num_samples, 2, figsize=(12, 4*num_samples))\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        idx = np.random.randint(0, y_true.shape[0])\n",
    "        \n",
    "        axes[i, 0].imshow(y_true[idx].reshape(32, 32), cmap='binary')\n",
    "        axes[i, 0].set_title(f'True - Sample {i+1}')\n",
    "        axes[i, 0].axis('off')\n",
    "        \n",
    "        axes[i, 1].imshow(y_pred[idx].reshape(32, 32), cmap='binary')\n",
    "        axes[i, 1].set_title(f'Predicted - Sample {i+1}')\n",
    "        axes[i, 1].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Main Execution\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Data Ingestion\n",
    "    data_dir = 'path/Data/Clean/timestamp_1'\n",
    "    target_size = (32, 12)\n",
    "    batch_size = 32\n",
    "    \n",
    "    print(f\"Contents of {data_dir}:\")\n",
    "    for root, dirs, files in os.walk(data_dir):\n",
    "        level = root.replace(data_dir, '').count(os.sep)\n",
    "        indent = ' ' * 4 * (level)\n",
    "        print(f\"{indent}{os.path.basename(root)}/\")\n",
    "        subindent = ' ' * 4 * (level + 1)\n",
    "        for f in files:\n",
    "            print(f\"{subindent}{f}\")\n",
    "    \n",
    "    print(\"\\nLoading dataset...\")\n",
    "    images, labels = load_dataset(data_dir, target_size)\n",
    "    \n",
    "    print(f\"\\nNumber of samples: {len(images)}\")\n",
    "    print(f\"Shape of first image stack: {images[0].shape if len(images) > 0 else 'No images'}\")\n",
    "    print(f\"Shape of first label: {labels[0].shape if len(labels) > 0 else 'No labels'}\")\n",
    "\n",
    "    if len(images) == 0 or len(labels) == 0:\n",
    "        print(\"Error: No images or labels were loaded. Please check the data directory and file structure.\")\n",
    "        exit(1)\n",
    "    \n",
    "    print(\"\\nPreparing data for model...\")\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test = prepare_data_for_model(images, labels)\n",
    "    \n",
    "    print(\"X_train shape:\", X_train.shape)\n",
    "    print(\"y_train shape:\", y_train.shape)\n",
    "    print(\"X_val shape:\", X_val.shape)\n",
    "    print(\"y_val shape:\", y_val.shape)\n",
    "\n",
    "    print(f\"\\nTraining samples: {len(X_train)}\")\n",
    "    print(f\"Validation samples: {len(X_val)}\")\n",
    "    print(f\"Test samples: {len(X_test)}\")\n",
    "    \n",
    "    # Model Creation and Training\n",
    "    input_shape = X_train.shape[1:]  # Dynamically set the input shape\n",
    "    num_classes = y_train.shape[1] \n",
    "    \n",
    "    model = create_custom_cnn(input_shape, num_classes)\n",
    "    model = compile_model(model)\n",
    "    \n",
    "    print(\"Training model...\")\n",
    "    history = train_model(model, X_train, y_train, X_val, y_val)\n",
    "    \n",
    "    # Model Evaluation\n",
    "    print(\"Evaluating model...\")\n",
    "    test_loss, test_accuracy, test_precision, test_recall = model.evaluate(X_test, y_test)\n",
    "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "    print(f\"Test Precision: {test_precision:.4f}\")\n",
    "    print(f\"Test Recall: {test_recall:.4f}\")\n",
    "    \n",
    "    # Generate predictions on test set\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Visualize Results\n",
    "    plot_training_history(history)\n",
    "    plot_precision_recall_curve(y_test, y_pred)\n",
    "    plot_roc_curve(y_test, y_pred)\n",
    "    visualize_predictions(y_test, y_pred)\n",
    "    \n",
    "    # Print best outcomes\n",
    "    print(f\"Best validation accuracy: {max(history.history['val_accuracy']):.4f}\")\n",
    "    print(f\"Best validation loss: {min(history.history['val_loss']):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
